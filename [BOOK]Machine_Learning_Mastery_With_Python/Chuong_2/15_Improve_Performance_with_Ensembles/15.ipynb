{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8b6291",
   "metadata": {},
   "source": [
    "# Chương 15: Cải Thiện Hiệu Suất với Ensembles\n",
    "\n",
    "Các phương pháp ensemble có thể giúp cải thiện độ chính xác cho tập dữ liệu của bạn. Trong chương này, bạn sẽ khám phá các phương pháp tạo ra một số kiểu ensemble mạnh mẽ nhất trong Python với scikit-learn. Bài học này sẽ hướng dẫn bạn về Boosting, Bagging và Majority Voting, giúp bạn tiếp tục nâng cao độ chính xác của các mô hình trên tập dữ liệu riêng. Sau khi hoàn thành bài học này, bạn sẽ biết:\n",
    "\n",
    "1. Cách sử dụng các phương pháp ensemble bagging như cây quyết định bagging, rừng ngẫu nhiên và extra trees.\n",
    "2. Cách sử dụng các phương pháp ensemble boosting như AdaBoost và stochastic gradient boosting.\n",
    "3. Cách sử dụng các phương pháp ensemble voting để kết hợp dự đoán từ nhiều thuật toán khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933217",
   "metadata": {},
   "source": [
    "## 15.1 Kết Hợp Các Mô Hình Thành Dự Đoán Ensemble\n",
    "\n",
    "Ba phương pháp phổ biến nhất để kết hợp dự đoán từ các mô hình khác nhau là:\n",
    "\n",
    "- **Bagging**: Xây dựng nhiều mô hình (thường cùng loại) từ các mẫu con khác nhau của tập dữ liệu huấn luyện.\n",
    "- **Boosting**: Xây dựng nhiều mô hình (thường cùng loại), mỗi mô hình học cách sửa lỗi dự đoán của mô hình trước đó trong chuỗi các mô hình.\n",
    "- **Voting**: Xây dựng nhiều mô hình (thường khác loại) và sử dụng các phép thống kê đơn giản (như tính trung bình) để kết hợp các dự đoán.\n",
    "\n",
    "Phần này giả định bạn đã quen thuộc với các thuật toán học máy và phương pháp ensemble và sẽ không đi sâu vào chi tiết cách thức hoạt động của các thuật toán hoặc tham số của chúng.\n",
    "\n",
    "Tập dữ liệu Pima Indians Diabetes được sử dụng để minh họa cho từng thuật toán. Mỗi thuật toán ensemble được minh họa bằng cách sử dụng cross validation 10-fold và độ chính xác phân loại làm chỉ số đánh giá hiệu suất."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662488fe",
   "metadata": {},
   "source": [
    "## 15.2 Thuật Toán Bagging\n",
    "\n",
    "**Bootstrap Aggregation** (hay Bagging) liên quan đến việc lấy nhiều mẫu từ tập dữ liệu huấn luyện của bạn (có thay thế) và huấn luyện một mô hình cho mỗi mẫu. Dự đoán đầu ra cuối cùng là trung bình của các dự đoán từ tất cả các mô hình con. Ba mô hình bagging được đề cập trong phần này bao gồm:\n",
    "\n",
    "- Bagged Decision Trees (Cây quyết định bagging)\n",
    "- Random Forest (Rừng ngẫu nhiên)\n",
    "- Extra Trees (Cây cực ngẫu nhiên)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7d8da-section",
   "metadata": {},
   "source": [
    "### 15.2.1 Bagged Decision Trees\n",
    "\n",
    "Bagging hoạt động tốt nhất với các thuật toán có phương sai cao. Một ví dụ phổ biến là cây quyết định, thường được xây dựng mà không cắt tỉa. Trong ví dụ dưới đây là cách sử dụng `BaggingClassifier` với thuật toán Classification and Regression Trees (DecisionTreeClassifier). Tổng cộng 100 cây được tạo ra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-forest-section",
   "metadata": {},
   "source": [
    "### 15.2.2 Random Forest\n",
    "\n",
    "Random Forest là một phần mở rộng của cây quyết định bagging. Các mẫu của tập dữ liệu huấn luyện được lấy có thay thế, nhưng các cây được xây dựng theo cách giảm sự tương quan giữa các bộ phân loại riêng lẻ. Cụ thể, thay vì tham lam chọn điểm phân chia tốt nhất trong quá trình xây dựng mỗi cây, chỉ một tập con ngẫu nhiên các đặc trưng được xem xét cho mỗi lần phân chia. Bạn có thể xây dựng mô hình Random Forest cho phân loại bằng lớp `RandomForestClassifier`. Ví dụ dưới đây minh họa việc sử dụng Random Forest cho phân loại với 100 cây và các điểm phân chia được chọn từ tập hợp ngẫu nhiên gồm 3 đặc trưng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-trees-section",
   "metadata": {},
   "source": [
    "### 15.2.3 Extra Trees\n",
    "\n",
    "Extra Trees là một biến thể khác của bagging, trong đó các cây ngẫu nhiên được xây dựng từ các mẫu của tập dữ liệu huấn luyện. Bạn có thể xây dựng mô hình Extra Trees cho phân loại bằng lớp `ExtraTreesClassifier`. Ví dụ dưới đây minh họa Extra Trees với số lượng cây được đặt là 100 và các điểm phân chia được chọn từ 7 đặc trưng ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd112c79",
   "metadata": {},
   "source": [
    "## 15.3 Thuật Toán Boosting\n",
    "\n",
    "**Thuật toán ensemble Boosting** tạo ra một chuỗi các mô hình cố gắng sửa chữa lỗi của các mô hình đứng trước chúng trong chuỗi. Sau khi được tạo ra, các mô hình đưa ra dự đoán có thể được đánh trọng số theo độ chính xác đã chứng minh của chúng và kết quả được kết hợp để tạo ra dự đoán đầu ra cuối cùng. Hai thuật toán ensemble boosting phổ biến nhất là:\n",
    "\n",
    "- AdaBoost\n",
    "- Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaboost-section",
   "metadata": {},
   "source": [
    "### 15.3.1 AdaBoost\n",
    "\n",
    "AdaBoost có lẽ là thuật toán ensemble boosting thành công đầu tiên. Nhìn chung, nó hoạt động bằng cách đánh trọng số các trường hợp trong tập dữ liệu dựa trên mức độ dễ hay khó phân loại, cho phép thuật toán chú ý hơn hoặc ít hơn đến chúng trong quá trình xây dựng các mô hình tiếp theo. Bạn có thể xây dựng mô hình AdaBoost cho phân loại bằng lớp `AdaBoostClassifier`. Ví dụ dưới đây minh họa việc xây dựng 30 cây quyết định liên tiếp bằng thuật toán AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 30\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-boosting-section",
   "metadata": {},
   "source": [
    "### 15.3.2 Stochastic Gradient Boosting\n",
    "\n",
    "Stochastic Gradient Boosting (còn được gọi là Gradient Boosting Machines) là một trong những kỹ thuật ensemble tinh vi nhất. Đây cũng là một kỹ thuật đang chứng minh có lẽ là một trong những kỹ thuật tốt nhất hiện có để cải thiện hiệu suất thông qua ensembles. Bạn có thể xây dựng mô hình Gradient Boosting cho phân loại bằng lớp `GradientBoostingClassifier`. Ví dụ dưới đây minh họa Stochastic Gradient Boosting cho phân loại với 100 cây."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fbb642-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200a253",
   "metadata": {},
   "source": [
    "## 15.4 Voting Ensemble\n",
    "\n",
    "**Voting** là một trong những cách đơn giản nhất để kết hợp dự đoán từ nhiều thuật toán học máy. Nó hoạt động bằng cách đầu tiên tạo ra hai hoặc nhiều mô hình độc lập từ tập dữ liệu huấn luyện của bạn. Sau đó, một Voting Classifier có thể được sử dụng để đóng gói các mô hình của bạn và tính trung bình các dự đoán của các mô hình con khi được yêu cầu đưa ra dự đoán cho dữ liệu mới. Các dự đoán của các mô hình con có thể được đánh trọng số, nhưng việc chỉ định trọng số cho các bộ phân loại một cách thủ công hoặc thậm chí theo trực giác là khó khăn.\n",
    "\n",
    "Các phương pháp tiên tiến hơn có thể học cách đánh trọng số tốt nhất cho các dự đoán từ các mô hình con, nhưng phương pháp này được gọi là stacking (stacked aggregation) và hiện không được cung cấp trong scikit-learn.\n",
    "\n",
    "Bạn có thể tạo mô hình ensemble voting cho phân loại bằng lớp `VotingClassifier`. Đoạn mã dưới đây cung cấp một ví dụ về việc kết hợp các dự đoán của hồi quy logistic, cây phân loại và hồi quy, và máy vector hỗ trợ cho một bài toán phân loại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voting-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 15.5 Tóm Tắt\n",
    "\n",
    "Trong chương này, bạn đã khám phá các thuật toán ensemble học máy để cải thiện hiệu suất của các mô hình trên các bài toán của bạn. Bạn đã học về:\n",
    "\n",
    "- **Bagging Ensembles** bao gồm Bagged Decision Trees, Random Forest và Extra Trees.\n",
    "- **Boosting Ensembles** bao gồm AdaBoost và Stochastic Gradient Boosting.\n",
    "- **Voting Ensembles** để lấy trung bình các dự đoán cho bất kỳ mô hình tùy ý nào."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
