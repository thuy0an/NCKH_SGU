# Chương 8: Lựa Chọn Đặc Trưng Cho Học Máy

Các đặc trưng dữ liệu sử dụng để huấn luyện mô hình học máy có ảnh hưởng rất lớn đến hiệu suất có thể đạt được. Các đặc trưng không liên quan hoặc liên quan một phần có thể ảnh hưởng tiêu cực đến hiệu suất mô hình. Trong chương này, bạn sẽ khám phá các kỹ thuật lựa chọn đặc trưng tự động mà bạn có thể sử dụng để chuẩn bị dữ liệu học máy của mình trong Python với scikit-learn.

## 8.1 Lựa Chọn Đặc Trưng

**Lựa Chọn Đặc Trưng** là quá trình tự động chọn những đặc trưng trong dữ liệu của bạn có đóng góp nhiều nhất cho biến dự đoán hoặc đầu ra mà bạn quan tâm. Việc có các đặc trưng không liên quan trong dữ liệu có thể làm giảm độ chính xác của nhiều mô hình, đặc biệt là các thuật toán tuyến tính như hồi quy tuyến tính và hồi quy logistic.

**Lợi ích của Lựa Chọn Đặc Trưng:**
- **Giảm Overfitting**: Dữ liệu ít dư thừa đồng nghĩa với ít cơ hội đưa ra quyết định dựa trên nhiễu.
- **Cải Thiện Độ Chính Xác**: Dữ liệu ít gây hiểu lầm đồng nghĩa với độ chính xác mô hình được cải thiện.
- **Giảm Thời Gian Huấn Luyện**: Dữ liệu ít hơn đồng nghĩa với thuật toán huấn luyện nhanh hơn.

## 8.2 Lựa Chọn Đơn Biến

**Lựa Chọn Đơn Biến** sử dụng các kiểm định thống kê để chọn những đặc trưng có mối quan hệ mạnh nhất với biến đầu ra. Thư viện scikit-learn cung cấp lớp `SelectKBest` có thể được sử dụng với một loạt các kiểm định thống kê khác nhau để chọn một số lượng đặc trưng cụ thể.

### Ví dụ:
- Sử dụng kiểm định thống kê **chi-squared (chi2)** để chọn 4 đặc trưng tốt nhất từ bộ dữ liệu tiểu đường Pima Indians.

## 8.3 Loại Bỏ Đặc Trưng Đệ Quy (RFE)

**Loại Bỏ Đặc Trưng Đệ Quy (RFE)** hoạt động bằng cách loại bỏ đệ quy các thuộc tính và xây dựng mô hình trên những thuộc tính còn lại. Nó sử dụng độ chính xác của mô hình để xác định thuộc tính nào (và tổ hợp thuộc tính nào) đóng góp nhiều nhất vào việc dự đoán thuộc tính mục tiêu.

### Ví dụ:
- RFE sử dụng mô hình hồi quy logistic để chọn 3 đặc trưng hàng đầu. Việc lựa chọn thuật toán không quá quan trọng, miễn là thuật toán đó khéo léo và nhất quán.

## 8.4 Phân Tích Thành Phần Chính (PCA)

**Phân Tích Thành Phần Chính (PCA)** là một kỹ thuật giảm dữ liệu sử dụng đại số tuyến tính để chuyển đổi tập dữ liệu thành dạng nén. PCA cho phép bạn chọn số lượng thành phần chính trong kết quả chuyển đổi.

### Ví dụ:
- PCA chuyển đổi tập dữ liệu thành 3 thành phần chính, giảm chiều dữ liệu đồng thời giữ lại thông tin quan trọng.

## 8.5 Độ Quan Trọng Của Đặc Trưng

**Độ Quan Trọng Của Đặc Trưng** là một kỹ thuật được sử dụng để ước tính tầm quan trọng của các đặc trưng bằng cách sử dụng các mô hình như **Random Forest** và **Extra Trees**. Các mô hình này có thể cung cấp một điểm số cho mỗi đặc trưng, chỉ ra tầm quan trọng của nó trong việc dự đoán thuộc tính mục tiêu.

### Ví dụ:
- **ExtraTreesClassifier** cung cấp điểm số về tầm quan trọng của đặc trưng cho mỗi thuộc tính. Điểm số càng lớn, đặc trưng càng quan trọng.

## 8.6 Tóm Tắt

Trong chương này, bạn đã học về bốn kỹ thuật lựa chọn đặc trưng tự động khác nhau để chuẩn bị dữ liệu học máy trong Python với scikit-learn:

- **Lựa Chọn Đơn Biến**
- **Loại Bỏ Đặc Trưng Đệ Quy (RFE)**
- **Phân Tích Thành Phần Chính (PCA)**
- **Độ Quan Trọng Của Đặc Trưng**

### 8.6.1 Tiếp Theo

Tiếp theo, bạn sẽ khám phá các phương pháp lấy mẫu lại có thể được sử dụng để đánh giá hiệu suất của thuật toán học máy trên dữ liệu chưa từng thấy.