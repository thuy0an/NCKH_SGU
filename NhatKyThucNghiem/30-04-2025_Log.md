# Experiment Log - [30-04-2025]

## üéØ M·ª•c ti√™u
- TƒÉng s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng t·ª´ 1000 ƒë·∫∑c tr∆∞ng l√™n 5000 ƒë·∫∑c tr∆∞ng
- Th√™m ti·ªÅn x·ª≠ l√Ω c√°c k√≠ t·ª± s·ªë, lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
## üßæ M√¥ t·∫£ th√≠ nghi·ªám
- M√¥ h√¨nh s·ª≠ d·ª•ng: Logistic Regression, Multinomial Naive Bayes
- K·ªπ thu·∫≠t ti·ªÅn x·ª≠ l√Ω:
  - Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
  - Lo·∫°i b·ªè HTML tags v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
  - Lo·∫°i b·ªè c√°c k√≠ t·ª± s·ªë
  - Tokenize
  - Lo·∫°i b·ªè stopwords
  - Lemmatization (s·ª≠ d·ª•ng WordNetLemmatizer)
- Vector h√≥a: Bag of Words (s·ª≠ d·ª•ng CountVectorizer)

## üßë‚Äçüíª Code s·ª≠ d·ª•ng
- Train_LR_NB_BOW.py (th∆∞ m·ª•c Models)

## ‚úÖ K·∫øt qu·∫£/Th√†nh c√¥ng
### Logistic Regression
- Accuracy: 0.768
- Precision: 0.744
- Recall: 0.768
- F1-score: 0.748


### Naive Bayes
- Accuracy: 0.716
- Precision: 0.687
- Recall: 0.716
- F1-score: 0.691

- Nh·∫≠n x√©t: Kh√¥ng c√≥ s·ª± thay ƒë·ªïi qu√° l·ªõn so v·ªõi 1000 ƒë·∫∑c tr∆∞ng
- K·∫øt qu·∫£ Metrics ·ªïn ƒë·ªãnh, ph√π h·ª£p v·ªõi m·ª•c ti√™u ƒë√£ h∆∞·ªõng ƒë·∫øn

## ‚ùå Th·∫•t b·∫°i/V·∫•n ƒë·ªÅ g·∫∑p ph·∫£i
- Kh√¥ng c√≥ s·ª± thay ƒë·ªïi qu√° l·ªõn khi tƒÉng s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë·ªÉ th·ª±c hi·ªán BoW


## Ghi ch√∫ kh√°c
- Th·ª≠ nghi·ªám v·ªõi 2 m√¥ h√¨nh c√≤n l·∫°i l√† SVM v√† Linear Regression ƒë·ªÉ ƒë√°nh gi√° k·∫øt qu·∫£

## Code python c·ªßa ng√†y th·ª±c nghi·ªám
```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
import joblib
import os
import re
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from sklearn.model_selection import train_test_split
import pickle

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import os
print("Current working dir:", os.getcwd())


stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# 1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
train_preprocessed = pd.read_csv('NCKH_SGU/TrichXuatDacTrung/TrainPreProcess.csv')
test_data = pd.read_csv('NCKH_SGU/TrainAndTestData/test.csv')
feature_bow = pd.read_csv('NCKH_SGU/TrichXuatDacTrung/FeatureBoW.csv')

X_train = feature_bow  
y_train = train_preprocessed['Score'] 

def preprocess_Text(text):
    text = text.lower()
    text = re.sub(r'<br\s*/?>', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    words = word_tokenize(text)
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
    return " ".join(words)

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho test
test_data['Text_Cleaned'] = test_data['Text'].apply(preprocess_Text)

# Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng BoW cho test
vectorizer = CountVectorizer()
vectorizer.fit(train_preprocessed['Text_Cleaned'])
X_train = vectorizer.transform(train_preprocessed['Text_Cleaned'])
X_test = vectorizer.transform(test_data['Text_Cleaned'])
y_test = test_data['Score']

# 3. Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh Logistic Regression
print("Hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression...")
lr_model = LogisticRegression(max_iter=5000, random_state=42)
lr_model.fit(X_train, y_train)

# ƒê√°nh gi√° m√¥ h√¨nh Logistic Regression
lr_predictions = lr_model.predict(X_test)
lr_report = classification_report(y_test, lr_predictions, output_dict=True)

# Ensure the result directory exists
result_dir = os.path.join(os.getcwd(), 'Result')
os.makedirs(result_dir, exist_ok=True)

# Save Logistic Regression results
lr_result_path = os.path.join(result_dir, 'logistic_regression_results.txt')
with open(lr_result_path, 'w', encoding='utf-8') as f:
    f.write("K·∫øt qu·∫£ Logistic Regression:\n")
    f.write(f"Accuracy: {accuracy_score(y_test, lr_predictions):.3f}\n")
    f.write(f"Precision: {lr_report['weighted avg']['precision']:.3f}\n")
    f.write(f"Recall: {lr_report['weighted avg']['recall']:.3f}\n")
    f.write(f"F1-score: {lr_report['weighted avg']['f1-score']:.3f}\n")

print("\nK·∫øt qu·∫£ Logistic Regression:")
print(f"Accuracy: {accuracy_score(y_test, lr_predictions):.3f}")
print(f"Precision: {lr_report['weighted avg']['precision']:.3f}")
print(f"Recall: {lr_report['weighted avg']['recall']:.3f}")
print(f"F1-score: {lr_report['weighted avg']['f1-score']:.3f}")

# 4. Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh Naive Bayes
print("\nHu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes...")
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# ƒê√°nh gi√° m√¥ h√¨nh Naive Bayes
nb_predictions = nb_model.predict(X_test)
nb_report = classification_report(y_test, nb_predictions, output_dict=True)

# Save Naive Bayes results
nb_result_path = os.path.join(result_dir, 'naive_bayes_results.txt')
with open(nb_result_path, 'w', encoding='utf-8') as f:
    f.write("K·∫øt qu·∫£ Naive Bayes:\n")
    f.write(f"Accuracy: {accuracy_score(y_test, nb_predictions):.3f}\n")
    f.write(f"Precision: {nb_report['weighted avg']['precision']:.3f}\n")
    f.write(f"Recall: {nb_report['weighted avg']['recall']:.3f}\n")
    f.write(f"F1-score: {nb_report['weighted avg']['f1-score']:.3f}\n")

print("\nK·∫øt qu·∫£ Naive Bayes:")
print(f"Accuracy: {accuracy_score(y_test, nb_predictions):.3f}")
print(f"Precision: {nb_report['weighted avg']['precision']:.3f}")
print(f"Recall: {nb_report['weighted avg']['recall']:.3f}")
print(f"F1-score: {nb_report['weighted avg']['f1-score']:.3f}")
```
