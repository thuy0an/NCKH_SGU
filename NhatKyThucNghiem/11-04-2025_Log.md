# Experiment Log - [11-04-2025]

## üéØ M·ª•c ti√™u
- Th·ª≠ nghi·ªám v√† ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa hai m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n: Logistic Regression v√† Naive Bayes tr√™n t·∫≠p d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω.
- √Åp d·ª•ng k·ªπ thu·∫≠t BoW ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho vƒÉn b·∫£n.
- Ghi l·∫°i k·∫øt qu·∫£ ƒë√°nh gi√°, ƒë·ªô ch√≠nh x√°c, v√† c√°c ch·ªâ s·ªë F1-score.

## üßæ M√¥ t·∫£ th√≠ nghi·ªám
- M√¥ h√¨nh s·ª≠ d·ª•ng: Logistic Regression, Multinomial Naive Bayes
- K·ªπ thu·∫≠t ti·ªÅn x·ª≠ l√Ω:
  - Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
  - Lo·∫°i b·ªè HTML tags v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
  - Tokenize
  - Lo·∫°i b·ªè stopwords
  - Lemmatization (s·ª≠ d·ª•ng WordNetLemmatizer)
- Vector h√≥a: Bag of Words (s·ª≠ d·ª•ng CountVectorizer)

## üßë‚Äçüíª Code s·ª≠ d·ª•ng
- Train_LR_NB_BOW.py (th∆∞ m·ª•c Models)

## ‚úÖ K·∫øt qu·∫£/Th√†nh c√¥ng
### Logistic Regression
- Accuracy: 0.769
- Precision: 0.746
- Recall: 0.769
- F1-score: 0.749
- Nh·∫≠n x√©t: Logistic Regression ho·∫°t ƒë·ªông t·ªët h∆°n m·ªôt ch√∫t so v·ªõi Naive Bayes tr√™n c√πng t·∫≠p ƒë·∫∑c tr∆∞ng.

### Naive Bayes
- Accuracy: 0.717
- Precision: 0.687
- Recall: 0.717
- F1-score: 0.690
- Nh·∫≠n x√©t: Naive Bayes nhanh v√† ƒë∆°n gi·∫£n nh∆∞ng k√©m ch√≠nh x√°c h∆°n so v·ªõi Logistic Regression trong th√≠ nghi·ªám n√†y.

- K·∫øt qu·∫£ Metrics ·ªïn ƒë·ªãnh, ph√π h·ª£p v·ªõi m·ª•c ti√™u ƒë√£ h∆∞·ªõng ƒë·∫øn

## ‚ùå Th·∫•t b·∫°i/V·∫•n ƒë·ªÅ g·∫∑p ph·∫£i
- T·∫≠p d·ªØ li·ªáu test c√≥ m·ªôt s·ªë vƒÉn b·∫£n kh√¥ng x·ª≠ l√Ω t·ªët, c√≥ th·ªÉ ·∫£nh h∆∞·ªüng t·ªõi hi·ªáu qu·∫£ m√¥ h√¨nh.
- Ch∆∞a Train th·ª≠ v·ªõi c√°c m√¥ h√¨nh h·ªçc m√°y kh√°c


## Ghi ch√∫ kh√°c
- Th·ª≠ nghi·ªám v·ªõi c√°c m√¥ h√¨nh kh√°c ƒë·ªÉ t√¨m ra ƒë√¢u l√† m√¥ h√¨nh t·ªët nh·∫•t khi d√πng tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng BoW
- C√≥ th·ªÉ th·ª≠ m·ªü r·ªông ho·∫∑c thu h·∫πp s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng t·ªëi ƒëa trong m·ªói l·∫ßn train

## Code python c·ªßa ng√†y th·ª±c nghi·ªám
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
import joblib
import os
import re
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from sklearn.model_selection import train_test_split
import pickle

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import os
print("Current working dir:", os.getcwd())


stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# 1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
train_preprocessed = pd.read_csv('NCKH_SGU/TrichXuatDacTrung/TrainPreProcess.csv')
test_data = pd.read_csv('NCKH_SGU/TrainAndTestData/test.csv')
feature_bow = pd.read_csv('NCKH_SGU/TrichXuatDacTrung/FeatureBoW.csv')

X_train = feature_bow  
y_train = train_preprocessed['Score'] 

def preprocess_Text(text):
    text = text.lower()
    text = re.sub(r'<br\s*/?>', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    words = word_tokenize(text)
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
    return " ".join(words)

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho test
test_data['Text_Cleaned'] = test_data['Text'].apply(preprocess_Text)

# Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng BoW cho test
vectorizer = CountVectorizer()
vectorizer.fit(train_preprocessed['Text_Cleaned'])
X_train = vectorizer.transform(train_preprocessed['Text_Cleaned'])
X_test = vectorizer.transform(test_data['Text_Cleaned'])
y_test = test_data['Score']

# 3. Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh Logistic Regression
print("Hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression...")
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)

# ƒê√°nh gi√° m√¥ h√¨nh Logistic Regression
lr_predictions = lr_model.predict(X_test)
lr_report = classification_report(y_test, lr_predictions, output_dict=True)

# Ensure the result directory exists
result_dir = os.path.join(os.getcwd(), 'Result')
os.makedirs(result_dir, exist_ok=True)

# Save Logistic Regression results
lr_result_path = os.path.join(result_dir, 'logistic_regression_results.txt')
with open(lr_result_path, 'w', encoding='utf-8') as f:
    f.write("K·∫øt qu·∫£ Logistic Regression:\n")
    f.write(f"Accuracy: {accuracy_score(y_test, lr_predictions):.3f}\n")
    f.write(f"Precision: {lr_report['weighted avg']['precision']:.3f}\n")
    f.write(f"Recall: {lr_report['weighted avg']['recall']:.3f}\n")
    f.write(f"F1-score: {lr_report['weighted avg']['f1-score']:.3f}\n")

print("\nK·∫øt qu·∫£ Logistic Regression:")
print(f"Accuracy: {accuracy_score(y_test, lr_predictions):.3f}")
print(f"Precision: {lr_report['weighted avg']['precision']:.3f}")
print(f"Recall: {lr_report['weighted avg']['recall']:.3f}")
print(f"F1-score: {lr_report['weighted avg']['f1-score']:.3f}")

# 4. Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh Naive Bayes
print("\nHu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes...")
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# ƒê√°nh gi√° m√¥ h√¨nh Naive Bayes
nb_predictions = nb_model.predict(X_test)
nb_report = classification_report(y_test, nb_predictions, output_dict=True)

# Save Naive Bayes results
nb_result_path = os.path.join(result_dir, 'naive_bayes_results.txt')
with open(nb_result_path, 'w', encoding='utf-8') as f:
    f.write("K·∫øt qu·∫£ Naive Bayes:\n")
    f.write(f"Accuracy: {accuracy_score(y_test, nb_predictions):.3f}\n")
    f.write(f"Precision: {nb_report['weighted avg']['precision']:.3f}\n")
    f.write(f"Recall: {nb_report['weighted avg']['recall']:.3f}\n")
    f.write(f"F1-score: {nb_report['weighted avg']['f1-score']:.3f}\n")

print("\nK·∫øt qu·∫£ Naive Bayes:")
print(f"Accuracy: {accuracy_score(y_test, nb_predictions):.3f}")
print(f"Precision: {nb_report['weighted avg']['precision']:.3f}")
print(f"Recall: {nb_report['weighted avg']['recall']:.3f}")
print(f"F1-score: {nb_report['weighted avg']['f1-score']:.3f}")
